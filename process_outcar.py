#!/usr/bin/env python3
"""
OUTCARå¤„ç†å‘½ä»¤è¡Œå·¥å…·
ä½¿ç”¨æ–¹æ³•: python process_outcar.py --help
"""

import argparse
import sys
import os
from pathlib import Path

# æ·»åŠ mlpotè·¯å¾„
current_dir = Path(__file__).parent
if current_dir.name == 'mlpot':
    sys.path.append(str(current_dir.parent))
else:
    sys.path.append(str(current_dir))

def process_outcar_files(args):
    """å¤„ç†OUTCARæ–‡ä»¶çš„ä¸»å‡½æ•°"""
    try:
        from mlpot.data.outcar_processor import OUTCARDatasetBuilder, find_outcar_files
        
        print("ğŸ”§ OUTCARæ–‡ä»¶å¤„ç†å·¥å…·")
        print("=" * 50)
        
        # æŸ¥æ‰¾OUTCARæ–‡ä»¶
        if args.files:
            outcar_files = args.files
            print(f"ğŸ“‹ ä½¿ç”¨æŒ‡å®šçš„ {len(outcar_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"ğŸ” åœ¨ {args.input_dir} ä¸­æœç´¢OUTCARæ–‡ä»¶...")
            outcar_files = find_outcar_files(args.input_dir, recursive=args.recursive)
            
        if not outcar_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°OUTCARæ–‡ä»¶")
            return False
            
        print(f"âœ… æ‰¾åˆ° {len(outcar_files)} ä¸ªOUTCARæ–‡ä»¶")
        
        if args.list_files:
            print("\nğŸ“‚ æ–‡ä»¶åˆ—è¡¨:")
            for i, f in enumerate(outcar_files, 1):
                print(f"  {i:3d}. {f}")
            if not args.process:
                return True
                
        if not args.process:
            print("\nğŸ’¡ æ·»åŠ  --process å‚æ•°å¼€å§‹å¤„ç†æ–‡ä»¶")
            return True
            
        # å¤„ç†æ–‡ä»¶
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ï¼Œè¾“å‡ºæ ¼å¼: {args.format}")
        
        builder = OUTCARDatasetBuilder(output_format=args.format, verbose=True)
        
        try:
            stats = builder.build_dataset(outcar_files, args.output)
            
            print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
            
            if args.show_stats:
                print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
                print(f"  - æ€»ç»“æ„æ•°: {stats['total_structures']}")
                print(f"  - æ€»åŸå­æ•°: {stats['total_atoms']}")
                print(f"  - å¹³å‡åŸå­æ•°/ç»“æ„: {stats['avg_atoms_per_structure']:.1f}")
                print(f"  - èƒ½é‡èŒƒå›´: {stats['energy_min']:.3f} ~ {stats['energy_max']:.3f} eV")
                print(f"  - æœ€å¤§åŠ›: {stats['force_max']:.3f} eV/Ã…")
                
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return False
            
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿mlpotæ¡†æ¶æ­£ç¡®å®‰è£…")
        return False


def create_training_script(args):
    """åˆ›å»ºè®­ç»ƒè„šæœ¬"""
    script_content = f'''#!/usr/bin/env python3
"""
ä½¿ç”¨{args.output}æ•°æ®é›†çš„mlpotè®­ç»ƒè„šæœ¬
è‡ªåŠ¨ç”ŸæˆäºOUTCARå¤„ç†å·¥å…·
"""

import sys
import torch
import torch.optim as optim
from pathlib import Path

# æ·»åŠ mlpotè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from mlpot import EquivariantNet
from mlpot.data.dataset import MolecularDataset, create_dataloader
from mlpot.core.trainer import PotentialTrainer
from mlpot.utils.helpers import set_random_seed

def main():
    # è®¾ç½®éšæœºç§å­
    set_random_seed(42)
    
    print("ğŸš€ å¼€å§‹mlpotè®­ç»ƒ")
    print("=" * 40)
    
    # 1. åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = MolecularDataset(
        data_path="{args.output}",
        format_type="{args.format}"
    )
    
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {{len(dataset)}} ä¸ªæ ·æœ¬")
    
    # 2. åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {{len(train_dataset)}} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {{len(val_dataset)}} æ ·æœ¬")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_dataloader(
        train_dataset, 
        batch_size={args.batch_size}, 
        shuffle=True,
        num_workers=2
    )
    
    val_loader = create_dataloader(
        val_dataset, 
        batch_size={args.batch_size}, 
        shuffle=False,
        num_workers=2
    )
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = EquivariantNet(
        hidden_dim={args.hidden_dim},
        num_layers={args.num_layers},
        cutoff_radius={args.cutoff},
        max_neighbors={args.max_neighbors}
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: {{total_params:,}} å‚æ•°")
    
    # 5. è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(), 
        lr={args.learning_rate},
        weight_decay=1e-5
    )
    
    # 6. åˆ›å»ºè®­ç»ƒå™¨
    trainer = PotentialTrainer(
        model=model,
        optimizer=optimizer,
        loss_config={{
            'energy_weight': {args.energy_weight},
            'force_weight': {args.force_weight},
            'loss_type': 'l1'
        }},
        gradient_clip=10.0
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    print(f"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ {{args.epochs}} ä¸ªepochs...")
    
    try:
        history = trainer.fit(
            train_loader, 
            val_loader, 
            epochs={args.epochs},
            save_checkpoint=True,
            checkpoint_dir="./checkpoints"
        )
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("ğŸ“ æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ ./checkpoints ç›®å½•")
        
    except KeyboardInterrupt:
        print("\\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {{e}}")

if __name__ == "__main__":
    main()
'''
    
    script_path = f"train_{Path(args.output).stem}.py"
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
        
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod(script_path, 0o755)
    
    print(f"ğŸ“ è®­ç»ƒè„šæœ¬å·²åˆ›å»º: {script_path}")
    print(f"ğŸ’¡ è¿è¡Œæ–¹æ³•: python {script_path}")


def main():
    parser = argparse.ArgumentParser(
        description="OUTCARæ–‡ä»¶å¤„ç†å·¥å…· - å°†VASP OUTCARæ–‡ä»¶è½¬æ¢ä¸ºmlpotæ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æœç´¢ç›®å½•ä¸­çš„OUTCARæ–‡ä»¶
  python process_outcar.py -i /path/to/calculations --list

  # å¤„ç†æ‰€æœ‰OUTCARæ–‡ä»¶å¹¶åˆ›å»ºæ•°æ®é›†
  python process_outcar.py -i /path/to/calculations -o dataset.h5 --process

  # å¤„ç†ç‰¹å®šæ–‡ä»¶
  python process_outcar.py --files OUTCAR1 OUTCAR2 -o dataset.h5 --process

  # ç”Ÿæˆè®­ç»ƒè„šæœ¬
  python process_outcar.py -i /path/to/calculations -o dataset.h5 --process --create-script
        """
    )
    
    # è¾“å…¥é€‰é¡¹
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        "-i", "--input-dir", type=str,
        help="åŒ…å«OUTCARæ–‡ä»¶çš„ç›®å½•"
    )
    input_group.add_argument(
        "--files", nargs="+", type=str,
        help="æŒ‡å®šç‰¹å®šçš„OUTCARæ–‡ä»¶è·¯å¾„"
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        "-o", "--output", type=str, default="outcar_dataset.h5",
        help="è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„ (é»˜è®¤: outcar_dataset.h5)"
    )
    
    parser.add_argument(
        "-f", "--format", type=str, default="h5",
        choices=['h5', 'pickle', 'npz'],
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: h5)"
    )
    
    # å¤„ç†é€‰é¡¹
    parser.add_argument(
        "-r", "--recursive", action="store_true",
        help="é€’å½’æœç´¢OUTCARæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--list", dest="list_files", action="store_true",
        help="åˆ—å‡ºæ‰¾åˆ°çš„OUTCARæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--process", action="store_true",
        help="å¼€å§‹å¤„ç†OUTCARæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--show-stats", action="store_true",
        help="æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"
    )
    
    # è®­ç»ƒè„šæœ¬ç”Ÿæˆé€‰é¡¹
    parser.add_argument(
        "--create-script", action="store_true",
        help="åˆ›å»ºè®­ç»ƒè„šæœ¬"
    )
    
    # è®­ç»ƒå‚æ•°ï¼ˆç”¨äºè„šæœ¬ç”Ÿæˆï¼‰
    train_group = parser.add_argument_group("è®­ç»ƒå‚æ•° (ç”¨äºè„šæœ¬ç”Ÿæˆ)")
    train_group.add_argument("--hidden-dim", type=int, default=128, help="éšè—å±‚ç»´åº¦")
    train_group.add_argument("--num-layers", type=int, default=3, help="ç½‘ç»œå±‚æ•°")
    train_group.add_argument("--cutoff", type=float, default=6.0, help="æˆªæ–­åŠå¾„")
    train_group.add_argument("--max-neighbors", type=int, default=50, help="æœ€å¤§é‚»å±…æ•°")
    train_group.add_argument("--batch-size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    train_group.add_argument("--learning-rate", type=float, default=0.001, help="å­¦ä¹ ç‡")
    train_group.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    train_group.add_argument("--energy-weight", type=float, default=1.0, help="èƒ½é‡æŸå¤±æƒé‡")
    train_group.add_argument("--force-weight", type=float, default=50.0, help="åŠ›æŸå¤±æƒé‡")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œé»˜è®¤åˆ—å‡ºæ–‡ä»¶
    if not args.list_files and not args.process:
        args.list_files = True
    
    success = process_outcar_files(args)
    
    if success and args.process and args.create_script:
        create_training_script(args)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
