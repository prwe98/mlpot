"""
MLPotè®¾å¤‡é…ç½®ä¼˜åŒ–å™¨
æ”¯æŒMacè°ƒè¯•å’ŒCUDAæœåŠ¡å™¨éƒ¨ç½²çš„è‡ªåŠ¨åŒ–é…ç½®

AUTHOR: prwe98
TIME: 2025.6.10
"""

import torch
import platform
import warnings
from typing import Dict, Any, Optional, Tuple
import os
import subprocess
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeviceManager:
    """è®¾å¤‡ç®¡ç†å™¨ï¼šMacå¼€å‘ + CUDAæœåŠ¡å™¨éƒ¨ç½²ä¼˜åŒ–"""
    
    def __init__(self, debug_mode: bool = True, force_device: Optional[str] = None):
        """
        åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨
        
        Args:
            debug_mode: æ˜¯å¦ä¸ºè°ƒè¯•æ¨¡å¼ï¼ˆMacå¼€å‘ç¯å¢ƒï¼‰
            force_device: å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šè®¾å¤‡ ('cpu', 'cuda', 'mps')
        """
        self.debug_mode = debug_mode
        self.force_device = force_device
        self.system_info = self._get_system_info()
        self.device = self._select_optimal_device()
        self.config = self._get_optimized_config()
        
        logger.info(f"è®¾å¤‡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç³»ç»Ÿ: {self.system_info['platform']}")
        logger.info(f"   è®¾å¤‡: {self.device}")
        logger.info(f"   è°ƒè¯•æ¨¡å¼: {self.debug_mode}")
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'platform': platform.system(),
            'machine': platform.machine(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'mps_available': hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        }
        
        # æ·»åŠ CUDA_VISIBLE_DEVICESä¿¡æ¯
        if info['cuda_available']:
            visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', None)
            if visible_devices is not None and visible_devices.strip():
                try:
                    if ',' in visible_devices:
                        physical_gpu_ids = [int(x.strip()) for x in visible_devices.split(',') if x.strip()]
                    else:
                        physical_gpu_ids = [int(visible_devices.strip())]
                    info['cuda_visible_devices'] = visible_devices
                    info['physical_gpu_ids'] = physical_gpu_ids
                    info['gpu_allocation_source'] = 'cuda_visible_devices'
                except ValueError:
                    logger.warning(f"æ— æ³•è§£æCUDA_VISIBLE_DEVICES: {visible_devices}")
                    info['cuda_visible_devices'] = None
                    info['physical_gpu_ids'] = list(range(info['cuda_device_count']))
                    info['gpu_allocation_source'] = 'auto_detect'
            else:
                info['cuda_visible_devices'] = None
                info['physical_gpu_ids'] = list(range(info['cuda_device_count']))
                info['gpu_allocation_source'] = 'auto_detect'
        else:
            info['cuda_visible_devices'] = None
            info['physical_gpu_ids'] = []
            info['gpu_allocation_source'] = 'no_cuda'
        
        # Macä¸“ç”¨æ£€æµ‹
        if info['platform'] == 'Darwin':
            try:
                # æ£€æµ‹Apple Silicon
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                info['cpu_brand'] = result.stdout.strip()
                info['is_apple_silicon'] = 'Apple' in info['cpu_brand']
            except:
                info['cpu_brand'] = 'Unknown'
                info['is_apple_silicon'] = False
                
        return info
    
    def _select_optimal_device(self) -> torch.device:
        """é€‰æ‹©æœ€ä¼˜è®¾å¤‡"""
        if self.force_device:
            return torch.device(self.force_device)
        
        # CUDAæœåŠ¡å™¨ç¯å¢ƒ
        if self.system_info['cuda_available'] and not self.debug_mode:
            # ç”Ÿäº§ç¯å¢ƒä¼˜å…ˆä½¿ç”¨æœ€å¥½çš„GPU
            # è€ƒè™‘CUDA_VISIBLE_DEVICESè®¾ç½®
            best_gpu = self._select_best_gpu()
            return torch.device(f"cuda:{best_gpu}")
        
        # Macå¼€å‘ç¯å¢ƒ
        if self.system_info['platform'] == 'Darwin':
            # Apple Siliconä¼˜å…ˆä½¿ç”¨MPS
            if self.system_info['mps_available'] and self.system_info.get('is_apple_silicon', False):
                return torch.device('mps')
            else:
                return torch.device('cpu')
        
        # Linuxå¼€å‘ç¯å¢ƒ
        if self.system_info['cuda_available']:
            # ä½¿ç”¨PyTorch GPU 0ï¼ˆè€ƒè™‘CUDA_VISIBLE_DEVICESåçš„ç¬¬ä¸€ä¸ªGPUï¼‰
            return torch.device('cuda:0')
        
        # é»˜è®¤CPU
        return torch.device('cpu')
    
    def _select_best_gpu(self) -> int:
        """é€‰æ‹©æœ€ä½³GPUï¼ˆè€ƒè™‘CUDA_VISIBLE_DEVICESè®¾ç½®ï¼‰"""
        if not self.system_info['cuda_available']:
            return 0
        
        # è·å–å½“å‰å¯è§çš„GPUæ•°é‡ï¼ˆå·²è€ƒè™‘CUDA_VISIBLE_DEVICESï¼‰
        visible_gpu_count = torch.cuda.device_count()
        if visible_gpu_count == 0:
            return 0
            
        best_gpu = 0
        max_memory = 0
        
        # éå†æ‰€æœ‰å¯è§çš„GPUï¼ˆPyTorché‡æ–°ç¼–å·çš„0,1,2...ï¼‰
        for i in range(visible_gpu_count):
            try:
                # torch.cuda.get_device_properties(i) è·å–çš„æ˜¯PyTorchç¼–å·çš„GPU
                memory = torch.cuda.get_device_properties(i).total_memory
                if memory > max_memory:
                    max_memory = memory
                    best_gpu = i
            except RuntimeError as e:
                # å¦‚æœæ— æ³•è®¿é—®æŸä¸ªGPUï¼Œè·³è¿‡
                logger.warning(f"æ— æ³•è®¿é—®GPU {i}: {e}")
                continue
                
        logger.info(f"é€‰æ‹©æœ€ä½³GPU: PyTorch GPU {best_gpu} (å†…å­˜: {max_memory/1024**3:.1f}GB)")
        return best_gpu
                
        return best_gpu
    
    def _get_optimized_config(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–é…ç½®"""
        config = {
            'device': self.device,
            'mixed_precision': False,
            'gradient_clipping': 1.0,
            'pin_memory': True,
            'non_blocking': True,
        }
        
        # Macå¼€å‘ä¼˜åŒ–
        if self.debug_mode and self.system_info['platform'] == 'Darwin':
            config.update({
                'batch_size': 4,  # å°æ‰¹æ¬¡ç”¨äºè°ƒè¯•
                'num_workers': 2,  # Macä¸Šè¾ƒå°‘çš„worker
                'accumulate_grad_batches': 4,  # æ¢¯åº¦ç´¯ç§¯è¡¥å¿å°æ‰¹æ¬¡
                'pin_memory': False,  # Macä¸Šå¯èƒ½ä¸éœ€è¦
                'persistent_workers': False,
                'prefetch_factor': 2,
                'max_neighbors': 30,  # å‡å°‘é‚»å±…æ•°é‡
                'cutoff_radius': 5.0,  # è¾ƒå°çš„æˆªæ–­åŠå¾„
                'hidden_dim': 128,  # è¾ƒå°çš„éšè—ç»´åº¦
                'num_layers': 3,  # è¾ƒå°‘çš„å±‚æ•°
                'precision': 32,  # ä½¿ç”¨FP32ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            })
            
            # MPSç‰¹æ®Šé…ç½®
            if str(self.device) == 'mps':
                config.update({
                    'mixed_precision': False,  # MPSæš‚ä¸æ”¯æŒAMP
                    'compile_model': False,  # MPSå¯èƒ½ä¸æ”¯æŒtorch.compile
                })
        
        # CUDAæœåŠ¡å™¨ä¼˜åŒ–
        elif str(self.device).startswith('cuda'):
            config.update({
                'batch_size': 16,  # æ›´å¤§çš„æ‰¹æ¬¡
                'num_workers': 8,  # æ›´å¤šçš„worker
                'accumulate_grad_batches': 1,
                'pin_memory': True,
                'persistent_workers': True,
                'prefetch_factor': 4,
                'max_neighbors': 50,
                'cutoff_radius': 6.0,
                'hidden_dim': 512,  # æ›´å¤§çš„æ¨¡å‹
                'num_layers': 6,
                'mixed_precision': True,  # ä½¿ç”¨AMPåŠ é€Ÿ
                'compile_model': True,  # ä½¿ç”¨torch.compileä¼˜åŒ–
                'precision': 16,  # ä½¿ç”¨FP16
            })
            
            # å¤šGPUé…ç½®
            if self.system_info['cuda_device_count'] > 1:
                config.update({
                    'strategy': 'ddp',  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
                    'sync_batchnorm': True,
                })
        
        # CPUé…ç½®
        else:
            config.update({
                'batch_size': 2,
                'num_workers': 4,
                'accumulate_grad_batches': 8,
                'pin_memory': False,
                'max_neighbors': 20,
                'cutoff_radius': 4.0,
                'hidden_dim': 64,
                'num_layers': 2,
                'precision': 32,
            })
        
        return config
    
    def get_dataloader_config(self) -> Dict[str, Any]:
        """è·å–DataLoaderé…ç½®"""
        base_config = {
            'batch_size': self.config['batch_size'],
            'num_workers': self.config['num_workers'],
            'pin_memory': self.config['pin_memory'],
            'drop_last': True,
            'shuffle': True,
        }
        
        # æ·»åŠ é¢å¤–é…ç½®
        if 'persistent_workers' in self.config:
            base_config['persistent_workers'] = self.config['persistent_workers']
        if 'prefetch_factor' in self.config:
            base_config['prefetch_factor'] = self.config['prefetch_factor']
            
        return base_config
    
    def get_model_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        return {
            'hidden_dim': self.config['hidden_dim'],
            'num_layers': self.config['num_layers'],
            'max_neighbors': self.config['max_neighbors'],
            'cutoff_radius': self.config['cutoff_radius'],
            'device': self.device,
        }
    
    def get_training_config(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒé…ç½®"""
        return {
            'mixed_precision': self.config['mixed_precision'],
            'gradient_clipping': self.config['gradient_clipping'],
            'accumulate_grad_batches': self.config['accumulate_grad_batches'],
            'precision': self.config['precision'],
        }
    
    def optimize_for_deployment(self):
        """åˆ‡æ¢åˆ°éƒ¨ç½²æ¨¡å¼ï¼ˆCUDAæœåŠ¡å™¨ï¼‰"""
        logger.info("åˆ‡æ¢åˆ°éƒ¨ç½²æ¨¡å¼...")
        self.debug_mode = False
        self.device = self._select_optimal_device()
        self.config = self._get_optimized_config()
        logger.info(f"   æ–°è®¾å¤‡: {self.device}")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
    
    def optimize_for_debug(self):
        """åˆ‡æ¢åˆ°è°ƒè¯•æ¨¡å¼ï¼ˆMacå¼€å‘ï¼‰"""
        logger.info("åˆ‡æ¢åˆ°è°ƒè¯•æ¨¡å¼...")
        self.debug_mode = True
        self.device = self._select_optimal_device()
        self.config = self._get_optimized_config()
        logger.info(f"   æ–°è®¾å¤‡: {self.device}")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
    
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("\n" + "="*60)
        print("MLPotè®¾å¤‡é…ç½®ä¿¡æ¯")
        print("="*60)
        print(f"æ“ä½œç³»ç»Ÿ: {self.system_info['platform']} {self.system_info['machine']}")
        print(f"å¤„ç†å™¨: {self.system_info.get('cpu_brand', self.system_info['processor'])}")
        print(f"Pythonç‰ˆæœ¬: {self.system_info['python_version']}")
        print(f"PyTorchç‰ˆæœ¬: {self.system_info['torch_version']}")
        
        print(f"\nğŸ¯ è®¾å¤‡é…ç½®:")
        print(f"å½“å‰è®¾å¤‡: {self.device}")
        print(f"è°ƒè¯•æ¨¡å¼: {self.debug_mode}")
        
        print(f"\nâš¡ CUDAä¿¡æ¯:")
        print(f"CUDAå¯ç”¨: {self.system_info['cuda_available']}")
        if self.system_info['cuda_available']:
            print(f"CUDAç‰ˆæœ¬: {self.system_info['cuda_version']}")
            print(f"å¯è§GPUæ•°é‡: {self.system_info['cuda_device_count']}")
            
            # æ˜¾ç¤ºGPUåˆ†é…ä¿¡æ¯
            allocation_source = self.system_info.get('gpu_allocation_source', 'unknown')
            print(f"GPUåˆ†é…æ¥æº: {allocation_source}")
            
            if allocation_source == 'cuda_visible_devices':
                visible_devices = self.system_info.get('cuda_visible_devices')
                physical_ids = self.system_info.get('physical_gpu_ids', [])
                print(f"CUDA_VISIBLE_DEVICES: {visible_devices}")
                print(f"ç‰©ç†GPU ID: {physical_ids}")
                print(f"PyTorch GPU ID: {list(range(self.system_info['cuda_device_count']))}")
                print("è¯´æ˜: é˜Ÿåˆ—ç³»ç»Ÿæˆ–æ‰‹åŠ¨è®¾ç½®äº†GPUåˆ†é…")
            elif allocation_source == 'auto_detect':
                print("è¯´æ˜: è‡ªåŠ¨æ£€æµ‹åˆ°çš„æ‰€æœ‰GPU")
            
            # æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯
            for i in range(self.system_info['cuda_device_count']):
                try:
                    props = torch.cuda.get_device_properties(i)
                    memory_gb = props.total_memory / 1024**3
                    
                    if allocation_source == 'cuda_visible_devices':
                        physical_ids = self.system_info.get('physical_gpu_ids', [])
                        if i < len(physical_ids):
                            physical_id = physical_ids[i]
                            print(f"  PyTorch GPU {i} (ç‰©ç†GPU {physical_id}): {props.name} ({memory_gb:.1f}GB)")
                        else:
                            print(f"  PyTorch GPU {i}: {props.name} ({memory_gb:.1f}GB)")
                    else:
                        print(f"  GPU {i}: {props.name} ({memory_gb:.1f}GB)")
                except RuntimeError as e:
                    print(f"  GPU {i}: æ— æ³•è®¿é—® ({e})")
        
        if self.system_info['platform'] == 'Darwin':
            print(f"\nğŸ Macç‰¹æ®ŠåŠŸèƒ½:")
            print(f"Apple Silicon: {self.system_info.get('is_apple_silicon', False)}")
            print(f"MPSå¯ç”¨: {self.system_info['mps_available']}")
        
        print(f"\næ¨èé…ç½®:")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
        print(f"å·¥ä½œè¿›ç¨‹: {self.config['num_workers']}")
        print(f"æ¨¡å‹ç»´åº¦: {self.config['hidden_dim']}")
        print(f"å±‚æ•°: {self.config['num_layers']}")
        print(f"æ··åˆç²¾åº¦: {self.config['mixed_precision']}")
        print("="*60)
    
    def create_deployment_script(self, output_path: str = "deploy_config.py"):
        """åˆ›å»ºéƒ¨ç½²é…ç½®è„šæœ¬"""
        import datetime
        script_content = f'''"""
è‡ªåŠ¨ç”Ÿæˆçš„MLPotéƒ¨ç½²é…ç½®
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}
"""

import torch
from mlpot.device_config import DeviceManager

# åˆ›å»ºéƒ¨ç½²è®¾å¤‡ç®¡ç†å™¨
device_manager = DeviceManager(debug_mode=False)

# è®¾å¤‡é…ç½®
DEVICE = device_manager.device
MODEL_CONFIG = device_manager.get_model_config()
TRAINING_CONFIG = device_manager.get_training_config()
DATALOADER_CONFIG = device_manager.get_dataloader_config()

# æ¨¡å‹å‚æ•°
HIDDEN_DIM = {self.config['hidden_dim']}
NUM_LAYERS = {self.config['num_layers']}
BATCH_SIZE = {self.config['batch_size']}
MIXED_PRECISION = {self.config['mixed_precision']}

print(f"éƒ¨ç½²è®¾å¤‡: {{DEVICE}}")
print(f"æ‰¹æ¬¡å¤§å°: {{BATCH_SIZE}}")
print(f"æ··åˆç²¾åº¦: {{MIXED_PRECISION}}")
'''
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        logger.info(f"éƒ¨ç½²é…ç½®å·²ä¿å­˜åˆ°: {output_path}")


def get_device_manager(debug_mode: bool = None, force_device: str = None) -> DeviceManager:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è®¾å¤‡ç®¡ç†å™¨
    
    Args:
        debug_mode: è°ƒè¯•æ¨¡å¼ï¼ŒNoneæ—¶è‡ªåŠ¨æ£€æµ‹
        force_device: å¼ºåˆ¶ä½¿ç”¨çš„è®¾å¤‡
    
    Returns:
        é…ç½®å¥½çš„è®¾å¤‡ç®¡ç†å™¨
    """
    # è‡ªåŠ¨æ£€æµ‹è°ƒè¯•æ¨¡å¼
    if debug_mode is None:
        # æ£€æµ‹æ˜¯å¦åœ¨Macæˆ–å¼€å‘ç¯å¢ƒ
        is_mac = platform.system() == 'Darwin'
        is_interactive = False
        try:
            # å®‰å…¨æ£€æµ‹äº¤äº’å¼ç¯å¢ƒ
            is_interactive = hasattr(__builtins__, '__IPYTHON__') or 'ipykernel' in str(type(get_ipython()))
        except NameError:
            is_interactive = False
        debug_mode = is_mac or is_interactive
    
    return DeviceManager(debug_mode=debug_mode, force_device=force_device)


# ä¾¿æ·å‡½æ•°
def auto_device() -> torch.device:
    """è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡"""
    return get_device_manager().device


def auto_config() -> Dict[str, Any]:
    """è‡ªåŠ¨è·å–ä¼˜åŒ–é…ç½®"""
    return get_device_manager().config


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("MLPotè®¾å¤‡é…ç½®æ¼”ç¤º")
    
    # åˆ›å»ºè®¾å¤‡ç®¡ç†å™¨
    dm = get_device_manager()
    dm.print_system_info()
    
    # æµ‹è¯•æ¨¡å¼åˆ‡æ¢
    print("\n æµ‹è¯•æ¨¡å¼åˆ‡æ¢:")
    print("å½“å‰æ¨¡å¼:", "è°ƒè¯•" if dm.debug_mode else "éƒ¨ç½²")
    
    if dm.debug_mode:
        print("åˆ‡æ¢åˆ°éƒ¨ç½²æ¨¡å¼...")
        dm.optimize_for_deployment()
    else:
        print("åˆ‡æ¢åˆ°è°ƒè¯•æ¨¡å¼...")
        dm.optimize_for_debug()
    
    # åˆ›å»ºéƒ¨ç½²è„šæœ¬
    dm.create_deployment_script()
